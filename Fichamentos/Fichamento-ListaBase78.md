# On Evaluating the Efficiency of Source Code Generated by LLMs

C. Niu, T. Zhang, C. Li, B. Luo and V. Ng, "On Evaluating the Efficiency of Source Code Generated by LLMs," 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (Forge) Conference Acronym:, Lisbon, Portugal, 2024, pp. 103-107, doi: [10.1145/3650105.3652295](https://doi.org/10.1145/3650105.3652295).

## 1. Fichamento de Conteúdo

Este artigo aborda a eficiência do código-fonte gerado por _Large Language Models_ (LLMs), um aspecto frequentemente negligenciado em avaliações que se concentram primariamente na correção funcional. Os autores argumentam que um código mais eficiente pode levar a um desempenho superior e maior produtividade no desenvolvimento de _software_ assistido por LLMs. O estudo avalia a eficiência do código gerado por LLMs em dois _benchmarks_ de programação de nível de entrada, _HumanEval_ e MBPP (_Mostly Basic Programming Problems_), e em um conjunto de problemas mais complexos da plataforma _LeetCode_. Além disso, o artigo explora diferentes _prompts_ que podem levar os LLMs a gerar código mais eficiente. Os resultados indicam que _prompts_ simples são eficazes para problemas básicos, enquanto problemas complexos se beneficiam de _prompts_ do tipo "cadeia de pensamento". As contribuições do artigo incluem a avaliação da eficiência do código gerado por LLMs, a proposta de um _benchmark_ baseado no _LeetCode_ para comparar a correção e eficiência de códigos mais complexos, e a investigação de técnicas de _prompting_ para otimizar a eficiência do código gerado. O estudo também revela que a capacidade de gerar código correto não está diretamente correlacionada com a capacidade de gerar código eficiente, e que um maior número de parâmetros em LLMs nem sempre garante melhor desempenho em eficiência. A estratégia de treinamento e os dados utilizados também influenciam significativamente a eficiência do código gerado.

## 2. Fichamento Bibliográfico

*   _Large Language Models (LLMs)_: Modelos de linguagem com capacidades notáveis para geração de código, que têm demonstrado excelência na criação de código correto e compatível (página 1).
*   _Eficiência do Código_: Refere-se ao desempenho e à otimização do código gerado, impactando diretamente a performance e a produtividade do software (página 1).
*   _HumanEval_: Um benchmark chave para medir a correção funcional de programas sintetizados a partir de descrições em linguagem natural, consistindo em 164 problemas de programação em Python (página 2).
*   _MBPP (Mostly Basic Programming Problems)_: Um conjunto de problemas de programação em Python, crowdsourced, projetado para ser solucionável por programadores iniciantes, cobrindo fundamentos de programação e funcionalidades da biblioteca padrão (página 2).
*   _LeetCodeEval_: Um dataset proposto pelos autores para avaliar a correção e eficiência do código gerado por LLMs, baseado em problemas da plataforma LeetCode, com diferentes níveis de dificuldade (fácil, médio e difícil) (página 2).
*   _Prompts_: Instruções ou entradas fornecidas aos LLMs para guiar a geração de código. O estudo explora como diferentes tipos de prompts (simples vs. cadeia de pensamento) afetam a eficiência do código gerado (página 1).
*   _Runtime (Tempo de Execução)_: Métrica principal utilizada para medir a eficiência do código gerado pelos LLMs, comparando o tempo que o código leva para ser executado em diferentes benchmarks (página 2).

## 3. Fichamento de Citações

*   _"Recent years have seen the remarkable capabilities of large language models (LLMs) for code generation. Different from existing work that evaluate the correctness of the code generated by LLMs, we propose to further evaluate its efficiency. More efficient code can lead to higher performance and execution efficiency of programs and software completed by LLM-assisted programming."_ 
*   _"However, the efficiency of the generated code is overlooked. In Figure 1, GPT-3.5 and CodeLlama’s solutions on the HumanEval/94 example both yield correct code. However, GPT-3.5’s solution exhibits higher running efficiency due to its O(√n) complexity compared to Code Llama’s O(n) complexity for determining prime numbers. This highlights the potential differences in execution efficiency among LLM-generated code."_ 
*   _"Consequently, in this paper, we propose to conduct an empirical study on the efficiency of LLM-generated code by investigating the following research questions (RQs): RQ1: How efficient is the code generated by LLMs? RQ2: How to prompt LLMs for more efficient code?"_ 
*   _"Results show that simple prompts enhance efficiency for basic problems, while complex problems benefit from a chain-of-thought prompt."_ 
*   _"First, the ability to generate correct code is not positively correlated with the ability to generate efficient code. For example, the Pass@10 of GPT-4 has a clear advantage over GPT-3.5, but the code generated by the former is not as efficient as the latter on both HumanEval and MBPP."_
*   _"Second, larger number of parameters does not promise higher performance. CodeLlama and WizardCoder series demonstrate that increasing the number of parameters does not significantly affect the runtime of generated code across models of different sizes."_ 
*   _"Then, training strategy and data have an impact on the efficiency of the generated code. For example, DeepSeek Coder 33B Instruct has a significant advantage over its Base version."_